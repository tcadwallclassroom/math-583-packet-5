---
title: "Packet 5 - Introducing Hypothesis Tests"
knit: (function(input_file, encoding) {
  out_dir <- 'docs';
  rmarkdown::render(input_file,
  encoding=encoding,
  output_file=file.path(dirname(input_file), out_dir, 'index.html'))})
author: "Todd CadwalladerOlsker"
date: "*Last updated:* `r Sys.Date()`"
output:
  rmdformats::downcute:
    downcute_theme: "chaos"
editor_options: 
  chunk_output_type: console
---

```{r setup, include=FALSE}
library(rmdformats)
library(openintro)
library(tidyverse)
library(gghighlight)
library(formatR)
knitr::opts_chunk$set(echo = T, cache = T, eval = T, cache.lazy = F, warning = F)
knitr::opts_chunk$set(tidy.opts=list(width.cutoff=80),tidy=F)
options(scipen = 999)
```

## Four-step Statistical Process

In other textbooks, you may read about a *four-step statistical process*: 

1. Ask a Question
2. Collect Data
3. Describe the Data
4. Make Inferences

At this point in the semester, you should definitely be thinking about your final project: You should already be thinking about a question to ask, and data that could be used to answer that question. 

So far, we have been talking about tools to *describe* data. Now, we will pivot into how we can *make inferences* using the data. You'll be using both sets of tools in your final project!

## Sophia's Coffee, revisited

In exam 1, I gave you a problem about Sophia claiming that she can determine whether cream was added to the coffee cup before the coffee, or afterwards:

"Sophia claims she can tell the difference in the taste of her coffee when the cream is poured into the cup first, rather than after the coffee... Grace believes that Sophia is just guessing, and is only correct 50% of the time. 

In order to put Sophia to the test, Grace pours 10 cups of coffee, some of which have the cream added to the cup first, some after pouring the coffee. Then, she asks Sophia to determine which have the cream added first."

*As an aside, this problem is based on a famous experiment by R. A. Fisher, one of the founders of modern statistical methods. He's also a very problematic figure, you'll read more about him is this packet's reading assignment.*

In that problem, we used Bayes' Theorem to determine the posterior probabilities of two competing models: one in which she is just guessing, and another in which she is correct 90% of the time.

Today we will examine the same problem, but rather than two competing models in which we know Sophia's accuracy in guessing, we will look at *traditional hypothesis testing*. Here, we have a *null hypothesis*:

\[H_0: \text{Sophia can determine when the cream was added with } 50\% \text{ accuracy.} \]

We want to test an *alternative hypothesis*. In this case, the alternative hypothesis might be that Sophia can determine when the cream was added with some accuracy, better than just guessing randomly:

\[H_A: \text{Sophia can determine when the cream was added with greater than } 50\% \text{ accuracy.} \]

In terms of a probability distribution, we can say $p$ is the probability that Sophia guesses correctly, and define our hypotheses in mathematical terms:

\begin{align*}
H_0: p &= .5 \\
H_A: p &> .5
\end{align*}

The idea is to *assume* that the null hypothesis is true, and then design a test: in this case, we'll pour ten cups of coffee, some of which have cream added first and some with cream added after, and ask Sophia to identify which is which. If she gets enough correct, then we will *reject the null hypothesis*: we no longer believe that she is just guessing, and she really has some ability to determine when the cream was added.

The key is the phrase, "enough correct" -- how many cups must Sophia correctly identify in order for us to reject the null hypothesis? 

There are two possible errors we can make here: The first, **Type I** error, is to reject the null hypothesis when Sophia really is just guessing. The second, **Type II** error, is to fail to reject the null hypothesis when Sophia really can tell the difference, at least, better than just guessing.

We usually want to control the Type I error. Traditionally, we want $\alpha$, the probability of a Type I error, to be less than or equal to 0.05. In this case, we want to set the bar for Sophia high enough that the probability of rejecting the null hypothesis when (or *given that*) the null hypotheses is actually true is no more than 0.05. 

Suppose Sophia really is just guessing, and truly is correct 50% of the time. Theoretically, this is a *binomial distribution problem*. The R functions `dbinom`, `pbinom`, and `qbinom` let us explore the binomial distribution:

```{r binom dist}
dbinom(8, 10, 0.5)
binom_dist <- data.frame(correct = c(0:10), dens = dbinom(0:10,10,0.5))

binom_dist %>% ggplot(aes(x = correct, y = dens)) +
  geom_col(color = "black", fill = "forestgreen") +
  scale_x_continuous(breaks = c(0:10)) 

binom_dist %>% ggplot(aes(x = correct, y = dens)) +
  geom_col(color = "black", fill = "forestgreen") +
  scale_x_continuous(breaks = c(0:10)) +
  gghighlight(correct >=8,
              unhighlighted_params = list(
                fill = "forestgreen",
                alpha = 0.25)) +
  annotate("text", 
           x = 8:10, 
           y = dbinom(8:10, 10, 0.5)+0.01,
           label = round(dbinom(8:10, 10, 0.5),4))

sum(dbinom(c(8:10), 10, 0.5))
sum(dbinom(c(9:10), 10, 0.5))

pbinom(8, 10, 0.5)
pbinom(8, 10, 0.5,lower.tail = F)

qbinom(0.05, 10, 0.5, lower.tail = F)
```

According to R, the cutoff value (or *critical value*) should be greater than 8. That is, we require Sophia to guess 9 or more coffees correctly in order to reject the null hypothesis.

To unpack this a bit, we can create our own R function for the binomial distribution:

```{r my binom}
my.binom <- function(success,total,prob = 0.5) {
  choose(total,success)*prob^success*(1-prob)^(total-success)
}

my.binom(8,10,0.5)
sum(my.binom(c(8:10),10,0.5))
sum(my.binom(c(9:10),10,0.5))
```

In modern statistics, we often take a kind of reverse approach: we can run our test first, and instead of comparing our result to a critical value, we calculate the *p-value* of our test result. The p-value is the probability of a test result at least as extreme as our test result, given the assumption that the null hypothesis is true. Using the above calculations, we can see that if Sophia is correct 8 times out of 10, the p-value is 0.055, whereas if she is correct 9 times out of 10, the p-value is 0.011. If we use 0.05 as our cutoff, then the result of 8 correct does not cause us to reject the null hypothesis, but a result of 9 does.

We often say that if our test has a p-value less than 0.05, then the result is *statistically significant*. However, 0.05 is arbitrary, and there may be good reasons for choosing a different cutoff level. It is common to report several "significance levels", or, as recommended recently by the ASA, to simply report the p-value itself. [Link to ASA Statement.](https://www.amstat.org/asa/files/pdfs/P-ValueStatement.pdf)

All of this is wrapped up in the R function, `binom.test()`. Try running the following code:
```{r binom.test, eval = FALSE}
binom.test(8,10,0.5,alternative = "greater")
```

Notice the confidence interval -- how is that calculated? This is a complicated question. The `binom.test` function uses something called the *Clopper-Pearson interval*. However, if the sample is large enough, we can use a normal approximation (which we will do later). 

## Simulation-based p-values

We can also run simulations, just like we did in the exam: The R code below will simulate 10,000 runs of this test, using the assumption of the null hypothesis. At the end, we can see how many of these simulations have a result at least as extreme as the result of our test: namely, that Sophia guessed 8 cups of coffee correctly. This lets us estimate the p-value by running simulations!

```{r coffee_sim}

simulations <- 10000    ### Number of simulated tests
num_cups <- 10          ### Number of cups of coffee per test
prob_correct <- .5      ### Probability that Sophia guesses correctly per cup


outcomes <- c("Correct", "Incorrect")
prob_vector <- c(prob_correct, 1 - prob_correct)

### Old version:

results <- numeric(simulations)

for(i in 1:simulations){
  num_correct <- 0 ### Keep track of the number of correct guesses per simulation
  for(j in 1:num_cups){
    guess <- sample(outcomes, 1, prob = prob_vector)
    if(guess == "Correct"){
      num_correct <- num_correct + 1
    } 
  }
  results[i] <- num_correct
}

results <- data.frame(results)
colnames(results) <- "correct"

### Alternative version, 3/15/2022

guess_correct <- numeric(simulations)

for(i in 1:simulations){
    guess <- sample(outcomes, 10, prob = prob_vector, replace = T)
    guess_correct[i] <- sum(guess == "Correct")
  }
results <- data.frame(guess_correct)
colnames(results) <- "correct"

###

glimpse(results)

results_table <- results$correct %>% table()
results_table

sims_as_extreme <- results %>% filter(correct >= 8) %>% nrow()
sims_as_extreme
p_val <- sims_as_extreme/simulations
p_val
```

In the code above, we have found `r sims_as_extreme` cases out of 10,000 in which we reject the null hypothesis when, in fact, Sophia only had a 50% chance of guessing correctly. This gives us a p-value of `r p_val` based on these simulations.

What we have just performed is sometimes called a *Monte Carlo* or *simulation-based* estimate of the p-value. This is really useful for situations where we may not have a convenient probability distribution to work with. 

## Point Estimates and Sampling Variance

According to 538, Joe Biden's average approval rating (as of March 10, 2022) is 42.5%. In other words, based on 538's weighting methodology and the results of recent polls, they estimate that 42.5% of American adults approve of Joe Biden's performance as president. For now, let's take this as a *parameter*: the proportion of American adults who approve really is $p = 0.425$.

In this section, we want to investigate the question: How close am I likely to be to the true parameter if I poll 100 adults?

Work with a partner to do the following in R:

1. Create a variable called `population_size`, and set it equal to the number of American adults, approximately 259 million.
2. Create a variable called `p` and set it equal to the parameter, proportion of the population that approves, 0.425.
3. Create a variable called `sample_size` and set it equal to 100.
4. Create a vector called `population` of size `population size`, with `population_size * p` entries that read `"approve"` and ``population_size * (1-p)` that read `"not"`.
5. Create a vector called `sample` that picks `sample_size` items from the `population` vector without replacement.
6. Use `sum(sample == "approve")` to find the number of people in the sample that approve, save this value as `sample_approve`.
7. Calculate a variable called `p_hat` that is equal to the proportion of people in the sample that approve.

Once you have code that accomplishes all of the above, save it as an R script, then run it several times. What different values of `p_hat` do you find?

Now, let's do this a bunch of times:

8. Create a variable called `simulations` and set it equal to 1000. 
9. Create a empty numeric vector called `p_hat_results` of size `simulations` 
10. Put your code for steps 5 through 7 into a for loop, so that you will run parts 5 through 7 `simulations` number of times, and save the results of `p_hat` for each simulation into `p_hat_results`. 
11. Now, turn the vector `p_hat_results` into a data frame called `results` with a single column called `p_hat`.
9. Create a histogram, with appropriate bin width, of these results. The x-axis should have `p_hat`, the sample proportions, and the y-axis should be the number of simulations. Set the limits of the x-axis as 0 to 1, with breaks every 0.1.
10. Use R to get the mean and standard deviation of `results$p_hat`.
11. Do the whole thing all over again, but now with `sample_size` set to 400. What changes do you see in your results?
12. Create several histograms, varying p from 0.10 to 0.90, and using sample sizes of 10, 50, 100, or 250. What trends do you see?

It appears as though the *sample proportion*, $\hat{p}$, is normally distributed. In fact, this is exactly the point of the *central limit theorem*. Before we use the central limit theorem, though, let's go back to a hypothesis test using randomization:

A recent (March 14, 2022) poll conducted by Schoen Cooperman Research surveyed 800 American adults, and found that 45% approved of Joe Biden's performance. Is there sufficient evidence to conclude that more than 42.5% of American adults approve of Joe Biden's performance?

In this case, our null hypothesis is that the approval rate is still $p=0.425$:

\begin{align*}
H_0: p &= 0.425 \\
H_A: p &> 0.425
\end{align*}

```{r sim}
population_size <- 259000000
p <- 0.425
x <- 0.45
sample_size <- 800

population <- c(rep("approve",population_size * p),
                rep("not",population_size * (1-p)))

simulations <- 100000
p_hat_results <- numeric(simulations)
for (i in 1:simulations) {
  sample <- sample(population, sample_size)
  sample_approve <- sum(sample == "approve")
  p_hat <- sample_approve / sample_size
  p_hat_results[i] <- p_hat
}
results <- data.frame(p_hat = p_hat_results)

results %>% ggplot(aes(x = p_hat)) +
  geom_histogram(color = "black", fill = "forestgreen", binwidth = 0.00125, center = p) +
  geom_vline(aes(xintercept = x), linetype = "dashed", color = "firebrick") +
  labs(
    title = "Distribution of Sampled Approval Rates",
    subtitle = paste("Using", simulations, "simulated sample proportions with p = ", p),
    x = "Sample proportion",
    y = "Number of simulations"
  ) +
  scale_x_continuous(limits = c(0.30,0.55), breaks = seq(0.35,0.50,0.05)) +
  gghighlight(p_hat >= x,
              unhighlighted_params = list(
                fill = "forestgreen",
                alpha = 0.25))

p_value <- sum(results$p_hat >= x)/simulations
p_value

```


## The Central Limit Theorem (for proportions)

If the sample size is large enough, then the sample proportion will tend to follow a normal distribution with 
\[\mu_{\hat{p}}=p \text{ and } SE_{\hat{p}} = \sqrt{\frac{p(1-p)}{n}}.\]
"Large enough" is determined by the *success-failure* conditions: \[np \ge 10 \text{ and } n(1-p)\ge 10.\]

(In practice, when we don't know $p$, we substitute our sample proportion, $\hat{p}$, instead.)

This allows us to use a normal distribution directly in a hypothesis test. Let's look at the same example as before:

A recent (March 14, 2022) poll conducted by Schoen Cooperman Research surveyed 800 American adults, and found that 45% approved of Joe Biden's performance. Is there sufficient evidence to conclude that more than 42.5% of American adults approve of Joe Biden's performance?

As a reminder, our null hypothesis is that the approval rate is still $p=0.425$:


\begin{align*}
H_0: p &= 0.425 \\
H_A: p &> 0.425
\end{align*}

We can calculate a $z-score$ for the observed sample: the z-score is the number of standard errors above (or below, in the case of a negative z-score) the mean. Based on the central limit theorem, the sample proportion $\hat{p}$ is normally distributed with a mean of $p = 0.425$ and standard error $SE = \sqrt{\frac{0.425(1-0.425)}{800}}$ -- as long as the success-failure conditions are met! (In this case, $0.425*800$ and $(1-0.425)*800$ are both greater than 10, so the success-failure conditions have been met.)

```{r hyp test}
n = 800
p = 0.425
se = sqrt(p*(1-p)/n)
x = 0.45
z = (x-p)/se
```

What does this z-score tell us? What we really want to know is what percentage of the normal distribution lies below this z-score: that tells us the probability of finding a result at least this extreme given the null hypothesis -- in other words, the p-value!

```{r normtail}
normTail(p, se, U = x, col = "forestgreen")
```

We can calculate this probability using the `pnorm` function:

```{r pnorm}
p_value <- pnorm(x, p, se, lower.tail = F)
p_value
### Or, a normalized version:
p_value <- pnorm(z, 0, 1, lower.tail = F)
p_value
```

Therefore, our p-value is approximately `r p_value`. Alternatively, if you want to go the other direction, and compute a critical value based on $\alpha=0.05$, you can use the qnorm function:

```{r qnorm}
qnorm(0.05, p, se, lower.tail = F)
```

We can also use a built-in R function:
```{r proptest}
prop.test(x = 0.45*800, n = 800, p = 0.425, 
          alternative = "greater", correct = F)
binom.test(x = 0.45*800, n = 800, p = 0.425, 
          alternative = "greater")
```


## Summary

We have talked about two ways to do a single proportion test: the normal single proportion test and the exact binomial test.

For each test, there are at least 3 ways to get a p-value:

1. Built-in "official" R functions
2. Using a known statistic/distribution and its associated formulas
3. Running simulations

Let's summarize these with one more example. Load the `openintro` package, and examine the data set `gss2010`. One of the variables is called `grass`, which asks respondents whether or not they think the use of marijuana should be made legal. Before continuing, make a guess: do you think the majority opinion is "legal", or "not legal"?

```{r gss2010}
library(openintro)
# glimpse(gss2010)
gss2010_grass <- gss2010 %>% drop_na(grass)
table(gss2010_grass$grass)
```

Is there enough evidence to conclude (at $\alpha = 0.05$) that a minority of Americans (in 2010) believed that the use of marijuana should be made legal?

### Using the normal single proportion test

1. Using R functions

```{r RFunctions}
x <- sum(gss2010_grass$grass == "LEGAL")
n <- sum(gss2010_grass$grass == "LEGAL")+sum(gss2010_grass$grass == "NOT LEGAL")
p <- 0.50

prop.test(x, n, p, correct = F, alternative = "less")

### We can also run two-tailed tests:
prop.test(x, n, p, correct = F, alternative = "two.sided")
```

2. Using Statistical Formulas

I'll also include some visualizations here, these are typically included in textbooks to help understand the ideas.

```{r Formulas, warning = FALSE}

x <- sum(gss2010_grass$grass == "LEGAL")
n <- sum(gss2010_grass$grass == "LEGAL")+sum(gss2010_grass$grass == "NOT LEGAL")
p <- 0.50

p_hat <- x/n
se <- sqrt(p*(1-p)/n)
z <- (p_hat-p)/se


pnorm(p_hat, p, se)
normTail(p, se, L = p_hat, col = "forestgreen")
```

3. Using simulations

```{r simulations}

### A simulated normal approximation (DO NOT ACTUALLY DO THIS)

x <- sum(gss2010_grass$grass == "LEGAL")
n <- sum(gss2010_grass$grass == "LEGAL")+sum(gss2010_grass$grass == "NOT LEGAL")
p <- 0.50

p_hat <- x/n
se <- sqrt(p*(1-p)/n)
z <- (p_hat-p)/se

simulations <- 100000
z_legal <- numeric(simulations)

for(i in 1:simulations){
  z_legal[i] <- rnorm(1,0,1)
}

### Alternative: z_legal <- rnorm(simulations,0,1)

results <- data.frame(z_legal = z_legal)

# glimpse(results)

sims_as_extreme <- results %>% filter(z_legal <= z) %>% nrow()
sims_as_extreme
p_val <- sims_as_extreme/simulations
p_val

results %>% ggplot(aes(x = z_legal)) +
  geom_histogram(color = "black", fill = "forestgreen", binwidth = 0.05) +
  scale_x_continuous(limits = c(-3.5,3.5)) +
  gghighlight(z_legal <= z,
              unhighlighted_params = list(
                fill = "forestgreen",
                alpha = 0.25))

```

### Exact Binomial Test

1. Using R functions


```{r binomtestsum}
x <- sum(gss2010_grass$grass == "LEGAL")
n <- sum(gss2010_grass$grass == "LEGAL")+sum(gss2010_grass$grass == "NOT LEGAL")
p <- 0.50

binom.test(x, n, p, alternative = "less")

### We can also run two-tailed tests:
binom.test(x, n, p, alternative = "two.sided")
```

2. Using Statistical Formulas

```{r mybinomsum}
x <- sum(gss2010_grass$grass == "LEGAL")
n <- sum(gss2010_grass$grass == "LEGAL")+sum(gss2010_grass$grass == "NOT LEGAL")
p <- 0.50

my.binom <- function(success,total,prob = 0.5) {
  choose(total,success)*prob^success*(1-prob)^(total-success)
}

sum(my.binom(0:x,n,p))
sum(dbinom(0:x,n,p))
pbinom(x,n,p)

binom_dist <- data.frame(legal = c(0:n), probs = dbinom(0:n,n,0.5))
binom_dist %>% ggplot(aes(x = legal, y = probs)) +
  geom_col(color = "black", fill = "forestgreen") +
  scale_x_continuous(limits = c(round(.45*n),round(.55*n))) +
  gghighlight(legal <=x,
              unhighlighted_params = list(
                fill = "forestgreen",
                alpha = 0.25)) 
```

3. Using simulations

```{r binomtestsimssum}

### Binomial Simulation

x <- sum(gss2010_grass$grass == "LEGAL")
n <- sum(gss2010_grass$grass == "LEGAL")+sum(gss2010_grass$grass == "NOT LEGAL")
p <- 0.50

prob_vector <- c(p,1-p)
outcomes <- c("LEGAL", "NOT LEGAL")
simulations <- 100000

num_legal <- numeric(simulations)

for(i in 1:simulations){
  response <- sample(outcomes, n, prob = prob_vector, replace = T)
  num_legal[i] <- sum(response == "LEGAL") 
}
### Alternative: num_legal <- rbinom(simulations, n, p)

results <- data.frame(legal = num_legal)

glimpse(results)

sims_as_extreme <- results %>% filter(legal <= x) %>% nrow()
sims_as_extreme
p_val <- sims_as_extreme/simulations
p_val

results %>% ggplot(aes(x = legal)) +
  geom_bar(color = "black", fill = "forestgreen") +
  scale_x_continuous(limits = c(round(.45*n),round(.55*n))) +
  gghighlight(legal <=x,
              unhighlighted_params = list(
                fill = "forestgreen",
                alpha = 0.25))

### Using a finite population without replacement

x <- sum(gss2010_grass$grass == "LEGAL")
n <- sum(gss2010_grass$grass == "LEGAL")+sum(gss2010_grass$grass == "NOT LEGAL")
p <- 0.50

prob_vector <- c(p,1-p)
population_size = 259000000
outcomes <- c(rep("LEGAL", p*population_size), rep("NOT LEGAL", (1-p)*population_size))
simulations <- 1000000

num_legal <- numeric(simulations)

for(i in 1:simulations){
  response <- sample(outcomes, n)
  num_legal[i] <- sum(response == "LEGAL")
}
results <- data.frame(legal = num_legal)

glimpse(results)

sims_as_extreme <- results %>% filter(legal <= x) %>% nrow()
sims_as_extreme
p_val <- sims_as_extreme/simulations
p_val

results %>% ggplot(aes(x = legal)) +
  geom_bar(color = "black", fill = "forestgreen") +
  scale_x_continuous(limits = c(round(.45*n),round(.55*n))) +
  gghighlight(legal <=x,
              unhighlighted_params = list(
                fill = "forestgreen",
                alpha = 0.25))
```

What are the pros and cons of each of these methods? Which one is the "best" method?

## To Turn In

On March 15, 2021, Germany suspended the use of the AstraZenica COVID-19 vaccine due to concerns that it may cause cerebral venous sinus thrombosis (CVST), a rare form of blood clot in the brain.

The initial concern was raised when 7 cases of CVST, all occurring within a month after vaccination, were reported to Germany's regulatory agency, the Paul-Ehrlich-Institute (PEI). At the time, Germany had administered 1.6 million does of the AstraZenica COVID-19 vaccine.

The decision to suspend the distribution of the AstraZenica vaccine has been widely criticized. Much of that criticism has come from comparisons to the rate of CVST in the general population. However, that rate is difficult to measure:

The "official" estimate is that CVST affects 2-5 adults per million per year. However, recent studies say this is likely an underestimate, and that the true rate may be as high as 15.7 per million per year. Turning this into a per-month estimate is a little tricky: CVST is correlated with infections, which occur more often in winter months like February and March. (See <https://www.dw.com/en/astrazeneca-whats-the-deal-with-thrombosis/a-56901525> for more.) 

Let's guesstimate the true rate of CVST for the time period of February 15-March 15 as \[\frac{10}{1,000,000}*\frac{2}{12}=\frac{1}{600,000}.\] I started with a mid-range estimate of 10 cases of CVST per million, divided by 12 months, times 2 to estimate that CVST is more likely to occur during winter than during summer. As a decimal, this rate is $0.0000016666...$

1. Calculate the rate of CVST (per million) among the 1.6 million adults who received the AstraZenica vaccine in one month in Germany. Does this appear to be higher than $0.000001666...$?

We want to test the hypothesis:

\begin{align*}
H_0: p &= 0.0000016667 \\
H_A: p &> 0.0000016667
\end{align*}

Our observed data is the 7 cases of CVST in 1.6 million German adults.

2. Check the success-failure conditions. Would it be appropriate to use a normal approximation test? Why or why not?

3. Use `qbinom` to find a critical value for our test at the $\alpha = 0.05$ level, and `pbinom` to find the p-value of our data. Then, run a `binom.test` of our hypothesis. What p-value do you get?

4. Run a simulation-based test. The population of Germany is approximately 54.4 million; find the probability that a randomly-selected sample of 1.6 million would have 7 or more cases of CVST, assuming the null hypotheses. Include a histogram, highlighting the region with 7 or more cases of CVST in the sample.

5. What conclusions can you draw? Do you agree with the German government's decision to suspend the use of the AstraZenica vaccine?

6. What flawed assumptions might we be making in the design of our hypothesis test?

## Reading Assignment

Read [How Eugenics Shaped Statistics](https://nautil.us/how-eugenics-shaped-statistics-9365/), by Aubrey Clayton. Write a short (2-3 paragraphs) reaction to this piece. As part of this reaction, you might consider questions like:

* Can we separate statistical methods from the people who created them?
* Should we abandon the notion of statistical significance?
* What does it really mean to be objective as a researcher?
