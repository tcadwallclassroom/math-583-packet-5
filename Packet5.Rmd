---
title: "Packet 5 - Introducing Hypothesis Tests"
knit: (function(input_file, encoding) {
  out_dir <- 'docs';
  rmarkdown::render(input_file,
  encoding=encoding,
  output_file=file.path(dirname(input_file), out_dir, 'index.html'))})
author: "Todd CadwalladerOlsker"
date: "*Last updated:* `r Sys.Date()`"
output:
  rmdformats::downcute:
    downcute_theme: "chaos"
editor_options: 
  chunk_output_type: console
---

```{r setup, include=FALSE}
library(rmdformats)
library(openintro)
library(tidyverse)
library(formatR)
knitr::opts_chunk$set(echo = TRUE, cache = TRUE, eval = TRUE)
knitr::opts_chunk$set(tidy.opts=list(width.cutoff=80),tidy=TRUE)
```

## Four-step Statistical Process

In other textbooks, you may read about a *four-step statistical process*: 

1. Ask a Question
2. Collect Data
3. Describe the Data
4. Make Inferences

At this point in the semester, you should definitely be thinking about your final project: You should already be thinking about a question to ask, and data that could be used to answer that question. 

So far, we have been talking about tools to *describe* data. Now, we will pivot into how we can *make inferences* using the data. You'll be using both sets of tools in your final project!

## Sophia's Coffee, revisited

In exam 1, I gave you a problem about Sophia claiming that she can determine whether cream was added to the coffee cup before the coffee, or afterwards:

"Sophia claims she can tell the difference in the taste of her coffee when the cream is poured into the cup first, rather than after the coffee... Grace believes that Sophia is just guessing, and is only correct 50% of the time. 

In order to put Sophia to the test, Grace pours 10 cups of coffee, some of which have the cream added to the cup first, some after pouring the coffee. Then, she asks Sophia to determine which have the cream added first."

*As an aside, this problem is based on a famous experiment by R. A. Fisher, one of the founders of modern statistical methods. He's also a very problematic figure, you'll read more about him is this packet's reading assignment.*

In that problem, we used Bayes' Theorem to determine the posterior probabilities of two competing models: one in which she is just guessing, and another in which she is correct 90% of the time.

Today we will examine the same problem, but rather than two competing models in which we know Sophia's accuracy in guessing, we will look at *traditional hypothesis testing*. Here, we have a *null hypothesis*:

\[H_0: \text{Sophia can determine when the cream was added with } 50\% \text{ accuracy.} \]

We want to test an *alternative hypothesis*. In this case, the alternative hypothesis might be that Sophia can determine when the cream was added with some accuracy, better than just guessing randomly:

\[H_A: \text{Sophia can determine when the cream was added with greater than } 50\% \text{ accuracy.} \]

In terms of a probability distribution, we can say $p$ is the probability that Sophia guesses correctly, and define our hypotheses in mathematical terms:

\begin{align*}
H_0: p &= .5 \\
H_A: p &> .5
\end{align*}

The idea is to *assume* that the null hypothesis is true, and then design a test: in this case, we'll pour ten cups of coffee, some of which have cream added first and some with cream added after, and ask Sophia to identify which is which. If she gets enough correct, then we will *reject the null hypothesis*: we no longer believe that she is just guessing, and she really has some ability to determine when the cream was added.

The key is the phrase, "enough correct" -- how many cups must Sophia correctly identify in order for us to reject the null hypothesis? 

There are two possible errors we can make here: The first, **Type I** error, is to reject the null hypothesis when Sophia really is just guessing. The second, **Type II** error, is to fail to reject the null hypothesis when Sophia really can tell the difference, at least, better than just guessing.

We usually want to control the Type I error. Traditionally, we want $\alpha$, the probability of a Type I error, to be less than or equal to 0.05. In this case, we want to set the bar for Sophia high enough that the probability of rejecting the null hypothesis when (or *given that*) the null hypotheses is actually true is no more than 0.05. 

Suppose Sophia really is just guessing, and truly is correct 50% of the time. Theoretically, this is a *binomial distribution problem*. The R functions `dbinom`, `pbinom`, and `qbinom` let us explore the binomial distribution:

```{r binom dist}
dbinom(8, 10, 0.5)
binom_dist <- data.frame(correct = c(0:10), dens = dbinom(0:10,10,0.5))

binom_dist %>% ggplot(aes(x = correct, y = dens)) +
  geom_col(color = "black", fill = "forestgreen") +
  scale_x_continuous(breaks = c(0:10))

sum(dbinom(c(8:10), 10, 0.5))
sum(dbinom(c(9:10), 10, 0.5))

pbinom(8, 10, 0.5)
pbinom(8, 10, 0.5,lower.tail = F)

qbinom(0.05, 10, 0.5, lower.tail = F)
```

According to R, the cutoff value (or *critical value*) should be greater than 8. That is, we require Sophia to guess 9 or more coffees correctly in order to reject the null hypothesis.

To unpack this a bit, we can create our own R function for the binomial distribution:

```{r my binom}
my.binom <- function(success,total,prob = 0.5) {
  choose(total,success)*prob^success*(1-prob)^(total-success)
}

my.binom(8,10,0.5)
sum(my.binom(c(8:10),10,0.5))
sum(my.binom(c(9:10),10,0.5))
```

In modern statistics, we often take a kind of reverse approach: we can run our test first, and instead of comparing our result to a critical value, we calculate the *p-value* of our test result. The p-value is the probability of a test result at least as extreme as our test result, given the assumption that the null hypothesis is true. Using the above calculations, we can see that if Sophia is correct 8 times out of 10, the p-value is 0.055, whereas if she is correct 9 times out of 10, the p-value is 0.011. If we use 0.05 as our cutoff, then the result of 8 correct does not cause us to reject the null hypothesis, but a result of 9 does.

We often say that if our test has a p-value less than 0.05, then the result is *statistically significant*. However, 0.05 is arbitrary, and there may be good reasons for choosing a different cutoff level. It is common to report several "significance levels", or, as recommended recently by the ASA, to simply report the p-value itself. [Link to ASA Statement.](https://www.amstat.org/asa/files/pdfs/P-ValueStatement.pdf)

All of this is wrapped up in the R function, `binom.test()`. Try running the following code:
```{r binom.test, eval = FALSE}
binom.test(8,10,0.5,alternative = "greater")
```

Notice the confidence interval -- how is that calculated? This is a complicated question. The `binom.test` function uses something called the *Clopper-Pearson interval*. If the sample is large enough, we can use a normal approximation (which we will do later). 

## Simulation-based p-values

We can also run simulations, just like we did in the exam: The R code below will simulate 10,000 runs of this test, using the assumption of the null hypothesis. At the end, we can see how many of these simulations have a result at least as extreme as the result of our test: namely, that Sophia guessed 8 cups of coffee correctly. This lets us estimate the p-value by running simulations!

```{r coffee_sim}

simulations <- 10000    ### Number of simulated tests
num_cups <- 10          ### Number of cups of coffee per test
prob_correct <- .5      ### Probability that Sophia guesses correctly per cup

results <- numeric(simulations)
outcomes <- c("Correct", "Incorrect")
prob_vector <- c(prob_correct, 1 - prob_correct)

for(i in 1:simulations){
  num_correct <- 0 ### Keep track of the number of correct guesses per simulation
  for(j in 1:num_cups){
    guess <- sample(outcomes, 1, prob = prob_vector)
    if(guess == "Correct"){
      num_correct <- num_correct + 1
    } 
  }
  results[i] <- num_correct
}

results <- data.frame(results)
colnames(results) <- "correct"
glimpse(results)

results_table <- results %>% table()
results_table

sims_as_extreme <- results %>% filter(correct >= 8) %>% nrow()
sims_as_extreme
p_val <- sims_as_extreme/simulations
p_val
```

In the code above, we have found `r sims_as_extreme` cases out of 10,000 in which we reject the null hypothesis when, in fact, Sophia only had a 50% chance of guessing correctly. This gives us a p-value of `r p_val` based on these simulations.

What we have just performed is sometimes called a *Monte Carlo* or *simulation-based* estimate of the p-value. This is really useful for situations where we may not have a convenient probability distribution to work with. 

## Point Estimates and Sampling Variance

According to 538, Joe Biden's average approval rating (as of March 10, 2022) is 45.2%. In other words, based on 538's weighting methodology and the results of recent polls, they estimate that 45.2% of American adults approve of Joe Biden's performance as president. For now, let's take this as a *parameter*: the proportion of American adults who approve really is $p = 45.2\%$.

In this section, we want to investigate the question: How close am I likely to be to the true parameter if I poll 100 adults?

Work with a partner to do the following in R:

1. Create a variable called `population_size`, and set it equal to the number of American adults, approximately 259 million.
2. Create a variable called `p` and set it equal to the parameter, proportion of the population that approves, 0.452.
3. Create a variable called `sample_size` and set it equal to 100.
4. Create a vector called `population` of size `population size`, with `population_size * p` entries that read `"approve"` and ``population_size * (1-p)` that read `"not"`.
5. Create a vector called `sample` that picks `sample_size` items from the `population` vector without replacement.
6. Use `sum(sample == "approve")` to find the number of people in the sample that approve, save this value as `sample_approve`.
7. Calculate a variable called `p_hat` that is equal to the proportion of people in the sample that approve.

Once you have code that accomplishes all of the above, save it as an R script, then run it several times. What different values of `p_hat` do you find?

Now, let's do this a bunch of times:

8. Create a variable called `simulations` and set it equal to 1000. 
9. Create a empty numeric vector called `p_hat_results` of size `simulations` 
10. Put the part of your code that completes steps 4 through 7 into a for loop, that will run parts 4 through 7 `simulations` number of times, and save the results of `p_hat` for each simulation into `p_hat_results`. 
11. Now, turn the vector `p_hat_results` into a data frame called `results` with a single column called `p_hat`.
9. Create a histogram, with appropriate bin width, of these results. The x-axis should have `p_hat`, the sample proportions, and the y-axis should be the number of simulations. Set the limits of the x-axis as 0 to 1, with breaks every 0.1.
10. Use R to get the mean and standard deviation of `results$p_hat`.
11. Do the whole thing all over again, but now with `sample_size` set to 400. What changes do you see in your results?
12. Create several histograms, varying p from 0.10 to 0.90, and using sample sizes of 10, 50, 100, or 250. What trends do you see?


